\documentclass[a4paper, 11pt]{article}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage[pdftex]{hyperref}

% Lengths and indenting
\setlength{\textwidth}{16.5cm}
\setlength{\marginparwidth}{1.5cm}
\setlength{\parindent}{0cm}
\setlength{\parskip}{0.15cm}
\setlength{\textheight}{22cm}
\setlength{\oddsidemargin}{0cm}
\setlength{\evensidemargin}{\oddsidemargin}
\setlength{\topmargin}{0cm}
\setlength{\headheight}{0cm}
\setlength{\headsep}{0cm}

\renewcommand{\familydefault}{\sfdefault}

\title{Machine Learning 2013: Project 1 - Regression Report}
\author{Diego Ballesteros (diegob@student.ethz.ch)\\ Jonas Nick (jnick@student.ethz.ch)\\ Rastislav Starkov (starkovr@student.ethz.ch)\\}
\date{\today}

\begin{document}
\maketitle

\section*{Experimental Protocol}

\begin{enumerate}
	\item Observe the data and analyze how the input features interact with the response variable (e.g. plotting individual curves) and identify outliers.
	\item Scale the data to fulfill the preconditions of some algorithms and obtain more stable numerical performance.
	\item Select the different tools and algorithms, e.g. libsvm, Matlab, R, mboost.
	\item Try the algorithms by using greedy backward feature selection with linear features and use some intuition and trial and error to identify possible transformations or important features. E.g. from linear regression we could identify some dominant features to use in the GAM regressor. Measure the performance using cross-validation.
In addition use hypothesis testing to get rid of superfluous variables.
    \item Identify the best tool and optimize its performance through cross-validation, use this as the final predictor.
\end{enumerate}

\section{Tools}
For the final solution, the language used was R with the following packages: stats, caret, party and mboost. RStudio was the IDE used during the development, testing and prediction.

\section{Algorithm}
For the final submission the method used was conditional inference trees with boosting. Conditional inference trees are a regression tool that allows for non-linear regression based on statistical significance of the input features and the response variable, this method is presented in \cite{Hothorn2005}. The main idea is to perform statistical tests on every split of the tree until the null hypothesis of independence between the response and the most influential variable can not be rejected, and continue the splitting by choosing the most influential variable on the response (e.g. determined by its p-value).

Even though the conditional inference trees provide a good result, there was room for improvement and this was done by using gradient boosting on the trees. Boosting is a useful learning technique, introduced by Schapire in \cite{Schapire:1990:SWL:83637.83645}, which allows ``weak'' learners to produce a strong learner by aggregating them after training each one on weighted versions of the original training data. In our case, the ``weak'' or base learner is the conditional inference tree. 

For the implementation we used the blackboost function from the mboost package \cite{Buehlmann:2008:StatSci}. Additionally, we scaled the input and output data to improve the numerical performance of the algorithms.

\section{Features}
For the final solution we used all the features because the conditional inference trees and boosting provide feature selection, the first one does it based on statistical significance of the covariates with the response and the second one by weighting the base learners based on the error on the training set.

\section{Parameters}
We performed cross validation on the number of boosting iterations to prevent overfitting. This was conveniently implemented in the cvrisk function from the mboost package in R. The cross-validation performed was 10-fold and the range of the boosting iterations was from 1 to 1000, the cost in this case was the RMSE. 

For the conditional inference tree we used the default parameters from the package because the boosting algorithm works better with weak learners \cite{Freund+Schapire:1996} so it is not recommendable to optimize the base learner through cross-validation.

\section{Lessons Learned}
We tried several other methods before trying the conditional inference trees, these were:

\begin{itemize}
	\item Ridge regression: Without input transformation or with the input variables log transformed, this performed poorly with CV(RMSE) of the order of 0.5 in 10-fold cross-validation in the training data. The results were barely above the easy baseline on the validation set.
	\item $\epsilon$-SVR: With a Gaussian kernel, for this learner we performed cross-validation to obtain optimal C and $\gamma$, for this the CV(RMSE) in 10-fold cross-validation was 0.36 which was also the cost in the validation set. This was discarded since it only achieved the easy baseline.
	\item Generalized linear models with boosting: This did not perform much better than the simple ridge regression, in this case boosting did not significantly improve the performance of the linear base learners.
    \item Generalized additive models: This was the second best learning method during our tests, in this case we did some analysis of how the input features influenced the response by looking at the information from the previous learners and determined a additive model as follows $Delay \leftarrow s(RFSize)+ BranchesAllowed + te(Depth, fL2Ucache)$\footnote{Where $s$ denote splines, a sum of basis functions estimated by the GAM. The $te$ term specifies a non-linear interaction between two variables (a tensor product)}  the cost during 10-fold cross-validation was $0.2$ and the cost in validation was $0.17$.
	
\end{itemize}

\bibliography{Report}
\bibliographystyle{acm}
\end{document} 
